{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "movie_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baYFZMW_bJHh",
        "outputId": "9302753c-f92c-4a8c-af8e-641d237a2896"
      },
      "source": [
        "!pip install -q tensorflow-text\n",
        "\n",
        "import collections\n",
        "import pathlib\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.3MB 28.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufneTVU1RNB8"
      },
      "source": [
        "BUFFER_SIZE = 40000\n",
        "BATCH_SIZE = 128\n",
        "VALIDATION_SIZE = 2000\n",
        "VOCAB_SIZE = 30000\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "\n",
        "EPOCHS = 10\n",
        "VALIDATION_STEP = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUtoed20cRJJ"
      },
      "source": [
        "# Load text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtnqhTuZm_B_"
      },
      "source": [
        "parent_dir = '/content/'\n",
        "\n",
        "FILE_NAMES = ['Animation.txt', 'Adventure.txt', 'Romance.txt', 'Comedy.txt', 'Action.txt', 'Family.txt', 'History.txt',\n",
        " 'Drama.txt', 'Crime.txt', 'Fantasy.txt', 'Science Fiction.txt', 'Thriller.txt', 'Music.txt', 'Horror.txt',\n",
        " 'Documentary.txt', 'Mystery.txt', 'Western.txt', 'TV Movie.txt', 'War.txt', 'Foreign.txt',]\n",
        "CLASSES = len(FILE_NAMES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvxBFA3Fbt1N"
      },
      "source": [
        "Converting the text files to data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0BjCOpOh7Ch",
        "outputId": "cc0bc64e-b15b-4831-9fd1-1ddaa6add3a2"
      },
      "source": [
        "def labeler(example, index):\n",
        "  return example, tf.cast(index, tf.int64)  \n",
        "\n",
        "labeled_data_sets = []\n",
        "\n",
        "for i, file_name in enumerate(FILE_NAMES):\n",
        "  print(i, file_name)\n",
        "  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\n",
        "  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
        "  labeled_data_sets.append(labeled_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Animation.txt\n",
            "1 Adventure.txt\n",
            "2 Romance.txt\n",
            "3 Comedy.txt\n",
            "4 Action.txt\n",
            "5 Family.txt\n",
            "6 History.txt\n",
            "7 Drama.txt\n",
            "8 Crime.txt\n",
            "9 Fantasy.txt\n",
            "10 Science Fiction.txt\n",
            "11 Thriller.txt\n",
            "12 Music.txt\n",
            "13 Horror.txt\n",
            "14 Documentary.txt\n",
            "15 Mystery.txt\n",
            "16 Western.txt\n",
            "17 TV Movie.txt\n",
            "18 War.txt\n",
            "19 Foreign.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8rBaeTYb7eC"
      },
      "source": [
        "Shuffle the loaded data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd544E-Sh63L"
      },
      "source": [
        "all_labeled_data = labeled_data_sets[0]\n",
        "for labeled_dataset in labeled_data_sets[1:]:\n",
        "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
        "  \n",
        "all_labeled_data = all_labeled_data.shuffle(BUFFER_SIZE, reshuffle_each_iteration=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83FYMRAEcL4a"
      },
      "source": [
        "Tokenize the labeled data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HdkxcSpccB2",
        "outputId": "160fa5b8-a03f-4544-bcd0-b92af67590a3"
      },
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "\n",
        "def tokenize(text, unused_label):\n",
        "  lower_case = tf_text.case_fold_utf8(text)\n",
        "  return tokenizer.tokenize(lower_case)\n",
        "\n",
        "tokenized_ds = all_labeled_data.map(tokenize)\n",
        "\n",
        "for text_batch in tokenized_ds.take(1):\n",
        "  print(\"Tokens: \", text_batch.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
            "Instructions for updating:\n",
            "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
            "Tokens:  [b'prairie' b'home' b'true' b'canadian' b'iconoclast' b'acclaimed'\n",
            " b'transgender' b'countryelectropop' b'artist' b'rae' b'spoon' b'revisits'\n",
            " b'stretches' b'rural' b'alberta' b'constituted' b'home' b'confronts'\n",
            " b'memories' b'growing' b'queer' b'abusive' b'evangelical' b'household']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3X71f878ss_"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def configure_dataset(dataset):\n",
        "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9qZkhwJcUNR"
      },
      "source": [
        "build a vocabulary by sorting tokens by frequency and keeping the top VOCAB_SIZE tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIM5J1aA575c",
        "outputId": "6184de5b-e0b5-40d0-d7ac-d84e96187108"
      },
      "source": [
        "tokenized_ds = configure_dataset(tokenized_ds)\n",
        "\n",
        "vocab_dict = collections.defaultdict(lambda: 0)\n",
        "for toks in tokenized_ds.as_numpy_iterator():\n",
        "  for tok in toks:\n",
        "    vocab_dict[tok] += 1\n",
        "\n",
        "vocab = sorted(vocab_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "vocab = [token for token, count in vocab]\n",
        "print(len(vocab))\n",
        "vocab = vocab[:VOCAB_SIZE]\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size: \", vocab_size)\n",
        "print(\"First five vocab entries:\", vocab[:30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "84795\n",
            "Vocab size:  30000\n",
            "First five vocab entries: [b'life', b'young', b'man', b'new', b'love', b'film', b'world', b'story', b'family', b'woman', b'find', b'time', b'father', b'years', b'girl', b'finds', b'war', b'wife', b'lives', b'home', b'friends', b'town', b'old', b'way', b'day', b'mother', b'school', b'people', b'son', b'help']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUgNcrw1fS2Q"
      },
      "source": [
        "Tokenize the data based on vocab vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeAzv-Eo59cO",
        "outputId": "8e97d649-4747-45c5-95bd-fcabd5ae2810"
      },
      "source": [
        "keys = vocab\n",
        "values = range(2, len(vocab) + 2)  # reserve 0 for padding, 1 for OOV\n",
        "\n",
        "init = tf.lookup.KeyValueTensorInitializer(\n",
        "    keys, values, key_dtype=tf.string, value_dtype=tf.int64)\n",
        "\n",
        "num_oov_buckets = 1\n",
        "vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov_buckets)\n",
        "\n",
        "def preprocess_text(text, label):\n",
        "  standardized = tf_text.case_fold_utf8(text)\n",
        "  tokenized = tokenizer.tokenize(standardized)\n",
        "  vectorized = vocab_table.lookup(tokenized)\n",
        "  return vectorized, label\n",
        "\n",
        "example_text, example_label = next(iter(all_labeled_data))\n",
        "print(\"Sentence: \", example_text.numpy())\n",
        "vectorized_text, example_label = preprocess_text(example_text, example_label)\n",
        "print(\"Vectorized sentence: \", vectorized_text.numpy())\n",
        "\n",
        "all_encoded_data = all_labeled_data.map(preprocess_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:  b'prairie home true canadian iconoclast acclaimed transgender countryelectropop artist rae spoon revisits stretches rural alberta constituted home confronts memories growing queer abusive evangelical household'\n",
            "Vectorized sentence:  [10111    21    72  1905 16868  1824 13576 30000   313 12125 18392  7405\n",
            " 13577   829 14476 26914    21  2490   807   645 11029  1992 11030  2132]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoAq3nD2fawj"
      },
      "source": [
        "Split the data for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlxQnvpxTUyJ"
      },
      "source": [
        "train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE)\n",
        "validation_data = all_encoded_data.take(VALIDATION_SIZE)\n",
        "\n",
        "train_data = train_data.padded_batch(BATCH_SIZE)\n",
        "validation_data = validation_data.padded_batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU39f2w_Tf1h",
        "outputId": "ad4af6ac-5c0b-4192-e6e5-6244cfc12257"
      },
      "source": [
        "sample_text, sample_labels = next(iter(validation_data))\n",
        "print(\"Text batch shape: \", sample_text.shape)\n",
        "print(\"Label batch shape: \", sample_labels.shape)\n",
        "print(\"First text example: \", sample_text[0])\n",
        "print(\"First label example: \", sample_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text batch shape:  (128, 98)\n",
            "Label batch shape:  (128,)\n",
            "First text example:  tf.Tensor(\n",
            "[10111    21    72  1905 16868  1824 13576 30000   313 12125 18392  7405\n",
            " 13577   829 14476 26914    21  2490   807   645 11029  1992 11030  2132\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0], shape=(98,), dtype=int64)\n",
            "First label example:  tf.Tensor(14, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl9AHo3BVNpv"
      },
      "source": [
        "vocab_size += 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nct4bSCe_QMp"
      },
      "source": [
        "train_data = configure_dataset(train_data)\n",
        "validation_data = configure_dataset(validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWvSNwaWfmoS"
      },
      "source": [
        "Create The training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahi4ySS4LwAF"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "      # preprocess_layer,\n",
        "      layers.Embedding(vocab_size, 32, mask_zero=True),\n",
        "      # layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "      # layers.Bidirectional(tf.keras.layers.LSTM(128,  return_sequences=True)),\n",
        "      # layers.Bidirectional(tf.keras.layers.LSTM(256,  return_sequences=True)),\n",
        "      # layers.Bidirectional(tf.keras.layers.LSTM(128,  return_sequences=True)),\n",
        "      layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "      layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation='relu'),\n",
        "      layers.Dropout(0.1),\n",
        "      # tf.keras.layers.Dense(CLASSES, activation='softmax')\n",
        "      layers.Dense(CLASSES)\n",
        "  ])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlscBezgqW2S"
      },
      "source": [
        "mc = tf.keras.callbacks.ModelCheckpoint('best_model', monitor='accuracy', mode='max', verbose=1, save_weights_only=False, save_best_only=True)\n",
        "mcv = tf.keras.callbacks.ModelCheckpoint('best_model_val', monitor='val_accuracy', mode='max', verbose=1, save_weights_only=False, save_best_only=True)\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5_mugjNKL08"
      },
      "source": [
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0LTasTzfrXZ"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeSE-YjdqAeN",
        "outputId": "2ee29640-b438-409e-ae33-cfe728569d86"
      },
      "source": [
        "history = model.fit(train_data,\n",
        "                    epochs=100,\n",
        "                    callbacks=[mcv, tensorboard_callback],\n",
        "                    validation_data=validation_data,\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "316/316 [==============================] - 104s 159ms/step - loss: 2.7909 - accuracy: 0.2689 - val_loss: 2.4554 - val_accuracy: 0.2890\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.28900, saving model to best_model_val\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/100\n",
            "316/316 [==============================] - 37s 116ms/step - loss: 2.4879 - accuracy: 0.2734 - val_loss: 2.3845 - val_accuracy: 0.2890\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.28900\n",
            "Epoch 3/100\n",
            "316/316 [==============================] - 36s 115ms/step - loss: 2.4582 - accuracy: 0.2700 - val_loss: 2.3603 - val_accuracy: 0.2890\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.28900\n",
            "Epoch 4/100\n",
            "316/316 [==============================] - 36s 115ms/step - loss: 2.4422 - accuracy: 0.2709 - val_loss: 2.3449 - val_accuracy: 0.2890\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.28900\n",
            "Epoch 5/100\n",
            "316/316 [==============================] - 37s 116ms/step - loss: 2.4309 - accuracy: 0.2673 - val_loss: 2.3339 - val_accuracy: 0.2890\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.28900\n",
            "Epoch 6/100\n",
            "316/316 [==============================] - 37s 116ms/step - loss: 2.4294 - accuracy: 0.2685 - val_loss: 2.3247 - val_accuracy: 0.2890\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.28900\n",
            "Epoch 7/100\n",
            "316/316 [==============================] - 37s 116ms/step - loss: 2.4142 - accuracy: 0.2679 - val_loss: 2.2958 - val_accuracy: 0.2890\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.28900\n",
            "Epoch 8/100\n",
            "316/316 [==============================] - 36s 114ms/step - loss: 2.3496 - accuracy: 0.2797 - val_loss: 2.2291 - val_accuracy: 0.3055\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.28900 to 0.30550, saving model to best_model_val\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9/100\n",
            "316/316 [==============================] - 36s 115ms/step - loss: 2.2871 - accuracy: 0.2911 - val_loss: 2.2007 - val_accuracy: 0.3020\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.30550\n",
            "Epoch 10/100\n",
            "316/316 [==============================] - 36s 115ms/step - loss: 2.2522 - accuracy: 0.2995 - val_loss: 2.1852 - val_accuracy: 0.3035\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.30550\n",
            "Epoch 11/100\n",
            "316/316 [==============================] - 36s 115ms/step - loss: 2.2265 - accuracy: 0.3064 - val_loss: 2.1755 - val_accuracy: 0.3085\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.30550 to 0.30850, saving model to best_model_val\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/100\n",
            "316/316 [==============================] - 36s 114ms/step - loss: 2.2021 - accuracy: 0.3143 - val_loss: 2.1669 - val_accuracy: 0.3135\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.30850 to 0.31350, saving model to best_model_val\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/100\n",
            "316/316 [==============================] - 36s 115ms/step - loss: 2.1812 - accuracy: 0.3221 - val_loss: 2.1614 - val_accuracy: 0.3190\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.31350 to 0.31900, saving model to best_model_val\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/100\n",
            "316/316 [==============================] - 37s 118ms/step - loss: 2.1634 - accuracy: 0.3277 - val_loss: 2.1593 - val_accuracy: 0.3190\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.31900\n",
            "Epoch 15/100\n",
            "316/316 [==============================] - 36s 113ms/step - loss: 2.1443 - accuracy: 0.3344 - val_loss: 2.1571 - val_accuracy: 0.3165\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.31900\n",
            "Epoch 16/100\n",
            "316/316 [==============================] - 36s 114ms/step - loss: 2.1259 - accuracy: 0.3410 - val_loss: 2.1560 - val_accuracy: 0.3190\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.31900\n",
            "Epoch 17/100\n",
            "316/316 [==============================] - 36s 114ms/step - loss: 2.1073 - accuracy: 0.3493 - val_loss: 2.1580 - val_accuracy: 0.3195\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.31900 to 0.31950, saving model to best_model_val\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/100\n",
            "316/316 [==============================] - 36s 115ms/step - loss: 2.0905 - accuracy: 0.3527 - val_loss: 2.1609 - val_accuracy: 0.3145\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.31950\n",
            "Epoch 19/100\n",
            "316/316 [==============================] - 36s 114ms/step - loss: 2.0747 - accuracy: 0.3621 - val_loss: 2.1641 - val_accuracy: 0.3175\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.31950\n",
            "Epoch 20/100\n",
            "316/316 [==============================] - 36s 113ms/step - loss: 2.0563 - accuracy: 0.3696 - val_loss: 2.1700 - val_accuracy: 0.3145\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.31950\n",
            "Epoch 21/100\n",
            "316/316 [==============================] - 36s 114ms/step - loss: 2.0391 - accuracy: 0.3741 - val_loss: 2.1732 - val_accuracy: 0.3205\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.31950 to 0.32050, saving model to best_model_val\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_model_val/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22/100\n",
            "316/316 [==============================] - 36s 113ms/step - loss: 2.0289 - accuracy: 0.3785 - val_loss: 2.1835 - val_accuracy: 0.3170\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.32050\n",
            "Epoch 23/100\n",
            "316/316 [==============================] - 36s 113ms/step - loss: 2.0090 - accuracy: 0.3856 - val_loss: 2.1917 - val_accuracy: 0.3130\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.32050\n",
            "Epoch 24/100\n",
            "316/316 [==============================] - 36s 114ms/step - loss: 1.9955 - accuracy: 0.3891 - val_loss: 2.2004 - val_accuracy: 0.3135\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.32050\n",
            "Epoch 25/100\n",
            "316/316 [==============================] - 36s 113ms/step - loss: 1.9799 - accuracy: 0.3959 - val_loss: 2.2094 - val_accuracy: 0.3135\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.32050\n",
            "Epoch 26/100\n",
            "316/316 [==============================] - 36s 114ms/step - loss: 1.9654 - accuracy: 0.4019 - val_loss: 2.2209 - val_accuracy: 0.3135\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.32050\n",
            "Epoch 27/100\n",
            "316/316 [==============================] - 36s 114ms/step - loss: 1.9486 - accuracy: 0.4079 - val_loss: 2.2316 - val_accuracy: 0.3120\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.32050\n",
            "Epoch 28/100\n",
            "316/316 [==============================] - 36s 113ms/step - loss: 1.9323 - accuracy: 0.4138 - val_loss: 2.2458 - val_accuracy: 0.3095\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.32050\n",
            "Epoch 29/100\n",
            "316/316 [==============================] - 36s 113ms/step - loss: 1.9171 - accuracy: 0.4237 - val_loss: 2.2520 - val_accuracy: 0.3080\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.32050\n",
            "Epoch 30/100\n",
            "316/316 [==============================] - 36s 112ms/step - loss: 1.9037 - accuracy: 0.4299 - val_loss: 2.2608 - val_accuracy: 0.3020\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.32050\n",
            "Epoch 31/100\n",
            "316/316 [==============================] - 36s 113ms/step - loss: 1.8934 - accuracy: 0.4346 - val_loss: 2.2737 - val_accuracy: 0.2985\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.32050\n",
            "Epoch 32/100\n",
            "316/316 [==============================] - 36s 113ms/step - loss: 1.8881 - accuracy: 0.4368 - val_loss: 2.3110 - val_accuracy: 0.3080\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.32050\n",
            "Epoch 33/100\n",
            "316/316 [==============================] - 36s 113ms/step - loss: 1.8780 - accuracy: 0.4410 - val_loss: 2.3033 - val_accuracy: 0.3105\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.32050\n",
            "Epoch 34/100\n",
            "316/316 [==============================] - 36s 112ms/step - loss: 1.8810 - accuracy: 0.4386 - val_loss: 2.3030 - val_accuracy: 0.3080\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.32050\n",
            "Epoch 35/100\n",
            "316/316 [==============================] - 36s 112ms/step - loss: 1.8831 - accuracy: 0.4414 - val_loss: 2.3042 - val_accuracy: 0.3060\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.32050\n",
            "Epoch 36/100\n",
            "316/316 [==============================] - 36s 112ms/step - loss: 1.8697 - accuracy: 0.4464 - val_loss: 2.3001 - val_accuracy: 0.3075\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.32050\n",
            "Epoch 37/100\n",
            "316/316 [==============================] - 36s 112ms/step - loss: 1.8501 - accuracy: 0.4526 - val_loss: 2.3079 - val_accuracy: 0.3015\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.32050\n",
            "Epoch 38/100\n",
            "316/316 [==============================] - 36s 113ms/step - loss: 1.8333 - accuracy: 0.4589 - val_loss: 2.3221 - val_accuracy: 0.3000\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.32050\n",
            "Epoch 39/100\n",
            "316/316 [==============================] - 36s 114ms/step - loss: 1.8240 - accuracy: 0.4636 - val_loss: 2.3341 - val_accuracy: 0.2985\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.32050\n",
            "Epoch 40/100\n",
            "316/316 [==============================] - 37s 116ms/step - loss: 1.8203 - accuracy: 0.4652 - val_loss: 2.3417 - val_accuracy: 0.2825\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.32050\n",
            "Epoch 41/100\n",
            "316/316 [==============================] - 37s 116ms/step - loss: 1.8183 - accuracy: 0.4690 - val_loss: 2.3486 - val_accuracy: 0.2760\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.32050\n",
            "Epoch 42/100\n",
            "316/316 [==============================] - 37s 116ms/step - loss: 1.8155 - accuracy: 0.4696 - val_loss: 2.3497 - val_accuracy: 0.2840\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.32050\n",
            "Epoch 43/100\n",
            "316/316 [==============================] - 37s 116ms/step - loss: 1.8047 - accuracy: 0.4731 - val_loss: 2.3516 - val_accuracy: 0.2925\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.32050\n",
            "Epoch 44/100\n",
            "316/316 [==============================] - 36s 115ms/step - loss: 1.7889 - accuracy: 0.4807 - val_loss: 2.3588 - val_accuracy: 0.2955\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.32050\n",
            "Epoch 45/100\n",
            "316/316 [==============================] - 37s 116ms/step - loss: 1.7760 - accuracy: 0.4846 - val_loss: 2.3685 - val_accuracy: 0.2945\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.32050\n",
            "Epoch 46/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.7673 - accuracy: 0.4860 - val_loss: 2.3760 - val_accuracy: 0.2935\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.32050\n",
            "Epoch 47/100\n",
            "316/316 [==============================] - 38s 119ms/step - loss: 1.7664 - accuracy: 0.4873 - val_loss: 2.3847 - val_accuracy: 0.2880\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.32050\n",
            "Epoch 48/100\n",
            "316/316 [==============================] - 38s 120ms/step - loss: 1.7638 - accuracy: 0.4880 - val_loss: 2.3834 - val_accuracy: 0.2865\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.32050\n",
            "Epoch 49/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.7528 - accuracy: 0.4933 - val_loss: 2.3982 - val_accuracy: 0.2725\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.32050\n",
            "Epoch 50/100\n",
            "316/316 [==============================] - 37s 118ms/step - loss: 1.7485 - accuracy: 0.4942 - val_loss: 2.4223 - val_accuracy: 0.2605\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.32050\n",
            "Epoch 51/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.7439 - accuracy: 0.4994 - val_loss: 2.4229 - val_accuracy: 0.2620\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.32050\n",
            "Epoch 52/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.7301 - accuracy: 0.5008 - val_loss: 2.4161 - val_accuracy: 0.2760\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.32050\n",
            "Epoch 53/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.7147 - accuracy: 0.5084 - val_loss: 2.4210 - val_accuracy: 0.2860\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.32050\n",
            "Epoch 54/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.6991 - accuracy: 0.5148 - val_loss: 2.4263 - val_accuracy: 0.2890\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.32050\n",
            "Epoch 55/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.6953 - accuracy: 0.5147 - val_loss: 2.4427 - val_accuracy: 0.2935\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.32050\n",
            "Epoch 56/100\n",
            "316/316 [==============================] - 38s 119ms/step - loss: 1.6986 - accuracy: 0.5151 - val_loss: 2.4520 - val_accuracy: 0.2930\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.32050\n",
            "Epoch 57/100\n",
            "316/316 [==============================] - 37s 118ms/step - loss: 1.7111 - accuracy: 0.5099 - val_loss: 2.4610 - val_accuracy: 0.2950\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.32050\n",
            "Epoch 58/100\n",
            "316/316 [==============================] - 38s 119ms/step - loss: 1.6992 - accuracy: 0.5131 - val_loss: 2.4608 - val_accuracy: 0.2945\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.32050\n",
            "Epoch 59/100\n",
            "316/316 [==============================] - 37s 115ms/step - loss: 1.6830 - accuracy: 0.5188 - val_loss: 2.4692 - val_accuracy: 0.2935\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.32050\n",
            "Epoch 60/100\n",
            "316/316 [==============================] - 37s 116ms/step - loss: 1.6709 - accuracy: 0.5234 - val_loss: 2.4836 - val_accuracy: 0.2970\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.32050\n",
            "Epoch 61/100\n",
            "316/316 [==============================] - 36s 115ms/step - loss: 1.6564 - accuracy: 0.5293 - val_loss: 2.5008 - val_accuracy: 0.2915\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.32050\n",
            "Epoch 62/100\n",
            "316/316 [==============================] - 37s 116ms/step - loss: 1.6431 - accuracy: 0.5337 - val_loss: 2.5123 - val_accuracy: 0.2915\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.32050\n",
            "Epoch 63/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.6393 - accuracy: 0.5359 - val_loss: 2.5150 - val_accuracy: 0.2855\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.32050\n",
            "Epoch 64/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.6469 - accuracy: 0.5315 - val_loss: 2.5043 - val_accuracy: 0.2895\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.32050\n",
            "Epoch 65/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.6519 - accuracy: 0.5293 - val_loss: 2.4967 - val_accuracy: 0.2845\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.32050\n",
            "Epoch 66/100\n",
            "316/316 [==============================] - 37s 116ms/step - loss: 1.6369 - accuracy: 0.5345 - val_loss: 2.4913 - val_accuracy: 0.2860\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.32050\n",
            "Epoch 67/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.6246 - accuracy: 0.5404 - val_loss: 2.5115 - val_accuracy: 0.2885\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.32050\n",
            "Epoch 68/100\n",
            "316/316 [==============================] - 37s 116ms/step - loss: 1.6091 - accuracy: 0.5427 - val_loss: 2.5383 - val_accuracy: 0.2840\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.32050\n",
            "Epoch 69/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.5977 - accuracy: 0.5462 - val_loss: 2.5594 - val_accuracy: 0.2910\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.32050\n",
            "Epoch 70/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.5911 - accuracy: 0.5486 - val_loss: 2.5779 - val_accuracy: 0.2875\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.32050\n",
            "Epoch 71/100\n",
            "316/316 [==============================] - 37s 117ms/step - loss: 1.5918 - accuracy: 0.5484 - val_loss: 2.5843 - val_accuracy: 0.2875\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.32050\n",
            "Epoch 72/100\n",
            "316/316 [==============================] - 37s 118ms/step - loss: 1.5940 - accuracy: 0.5476 - val_loss: 2.5821 - val_accuracy: 0.2915\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.32050\n",
            "Epoch 73/100\n",
            "211/316 [===================>..........] - ETA: 11s - loss: 1.5742 - accuracy: 0.5524"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArKI7DsVfuC5"
      },
      "source": [
        "Check the accuration of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vWJDNEXRVN6v"
      },
      "source": [
        "loss, accuracy = model.evaluate(validation_data)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmuSlpOkf1He"
      },
      "source": [
        "Add Prepocess layer to tokenize and encode text for predicition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cfJ2RGiwTiY7"
      },
      "source": [
        "# preprocess_layer = TextVectorization(\n",
        "#     max_tokens=vocab_size,\n",
        "#     # standardize=tf_text.case_fold_utf8,\n",
        "#     standardize='lower_and_strip_punctuation',\n",
        "#     split=tokenizer.tokenize,\n",
        "#     output_mode='int',\n",
        "#     output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
        "# preprocess_layer.set_vocabulary(vocab)\n",
        "\n",
        "preprocess_layer = TextVectorization(\n",
        "        standardize='lower_and_strip_punctuation',\n",
        "        max_tokens=vocab_size,\n",
        "        output_mode='int',\n",
        "        output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
        "preprocess_layer.set_vocabulary(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMOp0rt6gI24"
      },
      "source": [
        "Add the preprocess layer to the model and test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gawcWByMVR2D"
      },
      "source": [
        "best_model = tf.keras.models.load_model('best_model_val')\n",
        "\n",
        "export_model = tf.keras.Sequential(\n",
        "    [preprocess_layer, model,\n",
        "     layers.Activation('sigmoid')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZe0qxRXVWzJ"
      },
      "source": [
        "# Create a test dataset of raw strings\n",
        "test_ds = all_labeled_data.take(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
        "test_ds = configure_dataset(test_ds)\n",
        "loss, accuracy = export_model.evaluate(test_ds)\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XCRx-WsuVYxa"
      },
      "source": [
        "inputs = [\n",
        "    \"During a dangerous mission to stop a drug cartel operating between the US and Mexico, Kate Macer, an FBI agent, is exposed to some harsh realities.\",\n",
        "    \"Tony Montana and his close friend Manny, build a strong drug empire in Miami. However as his power begins to grow, so does his ego and his enemies, and his own paranoia begins to plague his empire\",\n",
        "    \"Cady joins a new public school and befriends Janis and Damian. They warn her to avoid the Plastics, a group led by Regina, but things get worse when she falls in love with Aaron, Regina's ex-lover\",  # Label: 0\n",
        "]\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = tf.argmax(predicted_scores, axis=1)\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2oNe8gThixrQ"
      },
      "source": [
        "export_model.save('mcg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz_pnOfrobnj"
      },
      "source": [
        "## plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b_fd6QTFGnZT"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_YYub0EDtwCu"
      },
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuiU5G6rrbw_"
      },
      "source": [
        "## Load models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i9fHw-Fcrh62"
      },
      "source": [
        "# The model weights (that are considered the best) are loaded into the model.\n",
        "new_model = tf.keras.models.load_model('mcg')\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JNb9wmtNrDPj"
      },
      "source": [
        "predicted_scores = new_model.predict(inputs)\n",
        "predicted_labels = tf.argmax(predicted_scores, axis=1)\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xMrRRnxXr0WN"
      },
      "source": [
        "test_ds = all_labeled_data.take(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
        "test_ds = configure_dataset(test_ds)\n",
        "\n",
        "loss, accuracy = new_model.evaluate(test_ds)\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE2beQpLgPR9"
      },
      "source": [
        "Download the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXrgPLbS83Uf"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-el_7S884J5"
      },
      "source": [
        "weights = model.get_layer('embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6qS_-Xs9Gbg"
      },
      "source": [
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = weights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcJgaAJ3YwH9"
      },
      "source": [
        "import shutil\n",
        "shutil.make_archive('/content/mcg/', 'zip', 'mcg')\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "   pass\n",
        "else:\n",
        "  files.download('vectors.tsv')\n",
        "  files.download('metadata.tsv')\n",
        "  files.download('/content/mcg.zip')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}